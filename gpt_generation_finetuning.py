import os
import argparse
import numpy as np
import torch
from datasets import load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    set_seed,
)

PROMPT = "The movie was"

def build_lm_dataset(ds, tokenizer, max_length=128):
    # Tokenize texts for causal LM
    def tok(batch):
        return tokenizer(
            batch["text"],
            truncation=True,
            max_length=max_length,
        )
    tokenized = ds.map(tok, batched=True, remove_columns=ds.column_names)
    # Trainer expects "labels" for loss computation; for causal LM labels=input_ids
    tokenized = tokenized.map(lambda x: {"labels": x["input_ids"]})
    tokenized.set_format("torch")
    return tokenized

def finetune_one(ds_text, save_dir, seed=42):
    os.makedirs(save_dir, exist_ok=True)
    set_seed(seed)

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained("gpt2")

    # GPT-2 has no pad token by default -> set to eos to allow batching
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.eos_token_id

    train_ds = build_lm_dataset(ds_text, tokenizer, max_length=128)

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    args = TrainingArguments(
        output_dir=save_dir,
        num_train_epochs=2,
        per_device_train_batch_size=1,      # safe for 4GB VRAM
        gradient_accumulation_steps=8,      # effective batch ~8
        learning_rate=5e-5,
        warmup_ratio=0.05,
        fp16=True,
        logging_steps=10,
        save_strategy="epoch",
        report_to="none",
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_ds,
        data_collator=data_collator,
    )

    trainer.train()
    trainer.save_model(save_dir)
    tokenizer.save_pretrained(save_dir)
    return model, tokenizer

@torch.no_grad()
def generate_samples(model, tokenizer, n=10, max_new_tokens=60):
    model.eval()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    inputs = tokenizer(
        PROMPT,
        return_tensors="pt",
        padding=True,
        truncation=True
    )
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    outputs = []
    for _ in range(n):
        gen_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,   # important (explain in report)
            do_sample=True,
            temperature=0.9,
            top_p=0.95,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.eos_token_id,
        )
        text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)
        outputs.append(text)
    return outputs

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(r"C:\Users\Win10\PycharmProjects\NLP-homework5-LLM\imdb_subset", type=str, required=True, help="path to imdb_subset")
    parser.add_argument(r"C:\Users\Win10\PycharmProjects\NLP-homework5-LLM\gpt2_output", type=str, required=True, help="path to reviews_generated.txt")
    parser.add_argument("--models_dir", type=str, required=True, help="directory to save finetuned models")
    args = parser.parse_args()

    subset = load_from_disk(args.subset_path)

    # 1 = positive, 0 = negative in IMDb
    pos = subset.filter(lambda x: x["label"] == 1).shuffle(seed=42).select(range(100))
    neg = subset.filter(lambda x: x["label"] == 0).shuffle(seed=42).select(range(100))

    pos_dir = os.path.join(args.models_dir, "gpt2_positive")
    neg_dir = os.path.join(args.models_dir, "gpt2_negative")

    print("[INFO] Fine-tuning POSITIVE model...")
    pos_model, pos_tok = finetune_one(pos, pos_dir)

    print("[INFO] Fine-tuning NEGATIVE model...")
    neg_model, neg_tok = finetune_one(neg, neg_dir)

    print("[INFO] Generating samples...")
    pos_gen = generate_samples(pos_model, pos_tok, n=10)
    neg_gen = generate_samples(neg_model, neg_tok, n=10)

    os.makedirs(os.path.dirname(args.output_file) or ".", exist_ok=True)
    with open(args.output_file, "w", encoding="utf-8") as f:
        f.write("Reviews generated by POSITIVE fine-tuned GPT-2:\n")
        for i, txt in enumerate(pos_gen, 1):
            f.write(f"\nReview {i}:\n{txt}\n")

        f.write("\n" + "="*60 + "\n")

        f.write("Reviews generated by NEGATIVE fine-tuned GPT-2:\n")
        for i, txt in enumerate(neg_gen, 1):
            f.write(f"\nReview {i}:\n{txt}\n")

    print(f"[DONE] Saved generations to: {args.output_file}")
    print(f"[DONE] Saved models to: {args.models_dir}")

if __name__ == "__main__":
    main()
