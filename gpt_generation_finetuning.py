import os
import argparse
import numpy as np
import torch
from datasets import load_from_disk
from transformers import (
    GPT2Tokenizer, GPT2LMHeadModel,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    set_seed,
)

PROMPT = "The movie was"

def build_lm_dataset(ds, tokenizer, max_length=150):
    def tok(batch):
        return tokenizer(
            batch["text"],
            truncation=True,
            max_length=max_length,
        )
    tokenized = ds.map(tok, batched=True, remove_columns=ds.column_names)
    tokenized = tokenized.map(lambda x: {"labels": x["input_ids"]})
    tokenized.set_format("torch")
    return tokenized

def finetune_one(ds_text, save_dir, seed=42):
    os.makedirs(save_dir, exist_ok=True)
    set_seed(seed)

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")

    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.eos_token_id

    train_ds = build_lm_dataset(ds_text, tokenizer, max_length=150)
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    training_args = TrainingArguments(
        output_dir=save_dir,
        num_train_epochs=3,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=5e-5,
        warmup_ratio=0.05,
        fp16=True,
        logging_steps=10,
        save_strategy="epoch",
        report_to="none",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        data_collator=data_collator,
    )

    trainer.train()


    trainer.model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)

    return model, tokenizer

@torch.no_grad()
def generate_samples(model, tokenizer, n=10, max_new_tokens=100):
    model.eval()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)


    input_ids = tokenizer.encode(PROMPT, return_tensors="pt").to(device)
    attention_mask = input_ids.ne(tokenizer.pad_token_id).to(device)

    outputs = []
    for _ in range(n):
        gen_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            do_sample=True,
            temperature=0.9,
            top_p=0.95,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.eos_token_id,
        )
        text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)
        outputs.append(text)
    return outputs

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--subset_path", type=str, required=True, help="path to imdb_subset")
    parser.add_argument("--output_file", type=str, required=True, help="path to reviews_generated.txt")
    parser.add_argument("--models_dir", type=str, required=True, help="directory to save finetuned models")
    args = parser.parse_args()

    subset = load_from_disk(args.subset_path)

    pos = subset.filter(lambda x: x["label"] == 1).shuffle(seed=42).select(range(150))
    neg = subset.filter(lambda x: x["label"] == 0).shuffle(seed=42).select(range(150))

    pos_dir = os.path.join(args.models_dir, "gpt2_positive")
    neg_dir = os.path.join(args.models_dir, "gpt2_negative")

   # print("[INFO] Fine-tuning POSITIVE model...")
    finetune_one(pos, pos_dir)

   # print("[INFO] Fine-tuning NEGATIVE model...")
    finetune_one(neg, neg_dir)


    pos_model = GPT2LMHeadModel.from_pretrained(pos_dir)
    pos_tok   = GPT2Tokenizer.from_pretrained(pos_dir)

    neg_model = GPT2LMHeadModel.from_pretrained(neg_dir)
    neg_tok   = GPT2Tokenizer.from_pretrained(neg_dir)


    pos_tok.pad_token = pos_tok.eos_token
    neg_tok.pad_token = neg_tok.eos_token

   # print("[INFO] Generating samples...")
    pos_gen = generate_samples(pos_model, pos_tok, n=10)
    neg_gen = generate_samples(neg_model, neg_tok, n=10)

    os.makedirs(os.path.dirname(args.output_file) or ".", exist_ok=True)
    with open(args.output_file, "w", encoding="utf-8") as f:
        f.write("Reviews generated by positive model:\n")
        for i, txt in enumerate(pos_gen, 1):
            f.write(f"Review {i}:\n{txt}\n")

        f.write("\n")

        f.write("Reviews generated by negative model:\n")
        for i, txt in enumerate(neg_gen, 1):
            f.write(f"Review {i}:\n{txt}\n")

   # print(f"[DONE] Saved generations to: {args.output_file}")
   # print(f"[DONE] Saved models to: {args.models_dir}")

if __name__ == "__main__":
    main()
